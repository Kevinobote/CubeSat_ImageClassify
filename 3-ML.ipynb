{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import joblib  # For saving models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image Processing\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Parallel Processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics & Evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully from NumPy files.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**efficiently preprocesses images** by normalizing, resizing, and converting them into a format that machine learning models can handle **with minimal memory usage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function (optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float32') / 255.0  # Normalize\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float32)\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research we have explored the utlisation of five machine learning models:\n",
    " \n",
    "- **Stochastic Gradient Descent (SGD) Classifier**: A linear model optimized using gradient descent, well-suited for large-scale datasets.  \n",
    "- **Support Vector Machine (SVM)**: A powerful classifier that finds the optimal decision boundary to separate different classes.  \n",
    "- **Random Forest Classifier**: An ensemble learning method that builds multiple decision trees to improve accuracy and reduce overfitting.  \n",
    "- **XGBoost Classifier**: A highly efficient gradient boosting algorithm that enhances predictive performance through boosting techniques.  \n",
    "- **Gradient Boosting Classifier**: A sequential ensemble learning method that builds trees iteratively to minimize errors and improve prediction accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Models directory set up.\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing with minimal memory usage\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(\"Models directory set up.\")\n",
    "\n",
    "\n",
    "# Define lightweight models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([(\"scaler\", StandardScaler()), (\"sgd\", SGDClassifier(loss=\"log_loss\", max_iter=50, tol=1e-3))]),\n",
    "    \"SVM\": Pipeline([(\"scaler\", StandardScaler()), (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50))]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=30, max_depth=4, min_samples_split=5, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=20, max_depth=3, learning_rate=0.1, verbosity=0, use_label_encoder=False)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD model training completed in 27.55s.\n",
      "SGD: Accuracy=0.4384, Train Time=27.55s, Model Size=224.35 KB\n",
      "Training SVM model...\n",
      "SVM model training completed in 1291.84s.\n",
      "SVM: Accuracy=0.4387, Train Time=1291.84s, Model Size=326480.53 KB\n",
      "Training RandomForest model...\n",
      "RandomForest model training completed in 3.99s.\n",
      "RandomForest: Accuracy=0.5397, Train Time=3.99s, Model Size=87.19 KB\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed in 35.34s.\n",
      "XGBoost: Accuracy=0.7173, Train Time=35.34s, Model Size=115.28 KB\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"{name} model training completed in {train_time:.2f}s.\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "\n",
    "    # Save model to \"models\" directory\n",
    "    model_path = os.path.join(models_dir, f\"{name}1.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # Convert to KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    \n",
    "    print(f\"{name}: Accuracy={acc:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing memory...\n",
      "Memory freed.\n",
      "Best model: XGBoost with Accuracy: 0.7173\n"
     ]
    }
   ],
   "source": [
    "# Free memory\n",
    "print(\"Freeing memory...\")\n",
    "del train_images, train_labels, val_images, val_labels, model\n",
    "gc.collect()  # Manually trigger garbage collection\n",
    "print(\"Memory freed.\")\n",
    "\n",
    "# Output best model\n",
    "best_model = max(results, key=lambda x: results[x][\"Accuracy\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function (optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float32') / 255.0  # Normalize\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float32)\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully from NumPy files.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing with minimal memory usage\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([(\"scaler\", MinMaxScaler()), (\"sgd\", SGDClassifier(loss=\"log_loss\", max_iter=11000, tol=1e-3))]),\n",
    "    \"SVM\": Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50, max_iter=5000))]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=5, n_jobs=-1),\n",
    "    # Optimized XGBoost model with improved hyperparameters\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.08, colsample_bytree=0.8, subsample=0.9, verbosity=0, use_label_encoder=False)\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD model...\n",
      "SGD model training completed in 20.01s.\n",
      "SGD: Accuracy=0.3707, Train Time=20.01s, Model Size=204.21 KB\n",
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model training completed in 421.04s.\n",
      "SVM: Accuracy=0.3812, Train Time=421.04s, Model Size=341956.81 KB\n",
      "Training RandomForest model...\n",
      "RandomForest model training completed in 0.28s.\n",
      "RandomForest: Accuracy=0.5397, Train Time=0.28s, Model Size=20.41 KB\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed in 72.21s.\n",
      "XGBoost: Accuracy=0.8665, Train Time=72.21s, Model Size=370.22 KB\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"{name} model training completed in {train_time:.2f}s.\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "\n",
    "    # Save model to \"models\" directory\n",
    "    model_path = os.path.join(models_dir, f\"{name}2.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # Convert to KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    \n",
    "    print(f\"{name}: Accuracy={acc:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing memory...\n",
      "Memory freed.\n",
      "Best model: XGBoost with Accuracy: 0.8665\n"
     ]
    }
   ],
   "source": [
    "# Free memory\n",
    "print(\"Freeing memory...\")\n",
    "del train_images, train_labels, val_images, val_labels, model\n",
    "gc.collect()  # Manually trigger garbage collection\n",
    "print(\"Memory freed.\")\n",
    "\n",
    "# Output best model\n",
    "best_model = max(results, key=lambda x: results[x][\"Accuracy\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully from NumPy files.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Models directory set up.\n",
      "Training SGD model...\n",
      "SGD model training completed in 292.89s.\n",
      "SGD: Accuracy=0.3892, Train Time=292.89s, Model Size=224.35 KB\n",
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model training completed in 381.53s.\n",
      "SVM: Accuracy=0.3812, Train Time=381.53s, Model Size=341956.81 KB\n",
      "Training RandomForest model...\n",
      "RandomForest model training completed in 0.21s.\n",
      "RandomForest: Accuracy=0.5440, Train Time=0.21s, Model Size=20.00 KB\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed in 52.03s.\n",
      "XGBoost: Accuracy=0.8940, Train Time=52.03s, Model Size=740.09 KB\n",
      "Freeing memory...\n",
      "Memory freed.\n",
      "Best model: XGBoost with Accuracy: 0.8940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocessing function (optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float32') / 255.0  # Normalize\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float32)\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully from NumPy files.\")\n",
    "\n",
    "# Apply preprocessing with minimal memory usage\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(\"Models directory set up.\")\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([(\"scaler\", StandardScaler()), (\"sgd\", SGDClassifier(loss=\"log_loss\", max_iter=10000, tol=1e-3))]),\n",
    "    \"SVM\": Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50, max_iter=5000))]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=5, n_jobs=-1),\n",
    "    # Further optimized XGBoost model with adjusted hyperparameters\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=75, max_depth=5, learning_rate=0.07, colsample_bytree=0.85, subsample=0.95, verbosity=0, use_label_encoder=False)\n",
    "\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"{name} model training completed in {train_time:.2f}s.\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "\n",
    "    # Save model to \"models\" directory\n",
    "    model_path = os.path.join(models_dir, f\"{name}3.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # Convert to KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    \n",
    "    print(f\"{name}: Accuracy={acc:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")\n",
    "\n",
    "# Free memory\n",
    "print(\"Freeing memory...\")\n",
    "del train_images, train_labels, val_images, val_labels, model\n",
    "gc.collect()  # Manually trigger garbage collection\n",
    "print(\"Memory freed.\")\n",
    "\n",
    "# Output best model\n",
    "best_model = max(results, key=lambda x: results[x][\"Accuracy\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Comparison  \n",
    "\n",
    "| Model         | Run | Accuracy | Train Time (s) | Model Size (KB) |\n",
    "|--------------|-----|----------|---------------|-----------------|\n",
    "| **SGD**      | 1   | 0.4384   | 27.55         | 224.35          |\n",
    "| **SVM**      | 1   | 0.4387   | 1291.84       | 326480.53       |\n",
    "| **RandomForest** | 1   | 0.5397   | 3.99          | 87.19           |\n",
    "| **XGBoost**  | 1   | 0.7173   | 35.34         | 115.28          |\n",
    "| **SVM**      | 2   | 0.3812   | 421.04        | 341956.81       |\n",
    "| **RandomForest** | 2   | 0.5397   | 0.28          | 20.41           |\n",
    "| **XGBoost**  | 2   | 0.8665   | 72.21         | 370.22          |\n",
    "| **SVM**      | 3   | 0.3812   | 381.53        | 341956.81       |\n",
    "| **RandomForest** | 3   | 0.5440   | 0.21          | 20.00           |\n",
    "| **XGBoost**  | 3   | 0.8940   | 52.03         | 740.09          |\n",
    "\n",
    "### Best Model  \n",
    "\n",
    "**XGBoost** emerges as the best model due to:  \n",
    "1. **Highest Accuracy**: 0.8940, outperforming other models.  \n",
    "2. **Reasonable Training Time**: Much faster than SVM while achieving better accuracy.  \n",
    "3. **Manageable Model Size**: Smaller than SVM, making it more efficient for deployment.  \n",
    "\n",
    "Given these factors, **XGBoost provides the best trade-off between accuracy, training efficiency, and storage size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More model Optimization, adjusting the hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully from NumPy files.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Models directory set up.\n",
      "Training SGD model...\n",
      "SGD model training completed in 36.70s.\n",
      "SGD: Accuracy=0.3676, Train Time=36.70s, Model Size=204.21 KB\n",
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model training completed in 332.88s.\n",
      "SVM: Accuracy=0.3695, Train Time=332.88s, Model Size=325040.61 KB\n",
      "Training RandomForest model...\n",
      "RandomForest model training completed in 0.75s.\n",
      "RandomForest: Accuracy=0.4940, Train Time=0.75s, Model Size=10.44 KB\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed in 109.52s.\n",
      "XGBoost: Accuracy=0.8499, Train Time=109.52s, Model Size=283.03 KB\n",
      "Freeing memory...\n",
      "Memory freed.\n",
      "Best model: XGBoost with Accuracy: 0.8499\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function (further optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float32') / 255.0  # Normalize\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float32)\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully from NumPy files.\")\n",
    "\n",
    "# Apply optimized preprocessing\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(\"Models directory set up.\")\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", MinMaxScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0001, penalty=\"l2\", max_iter=5000, tol=1e-4))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", MinMaxScaler()), \n",
    "        (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50, max_iter=3000))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=8, max_depth=2, min_samples_split=6, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=50, max_depth=3, learning_rate=0.08, colsample_bytree=0.8, subsample=0.9, verbosity=0, use_label_encoder=False\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    print(f\"{name} model training completed in {train_time:.2f}s.\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "\n",
    "    # Save model to \"models\" directory\n",
    "    model_path = os.path.join(models_dir, f\"{name}_opt.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # Convert to KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    \n",
    "    print(f\"{name}: Accuracy={acc:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")\n",
    "\n",
    "# Free memory\n",
    "print(\"Freeing memory...\")\n",
    "del train_images, train_labels, val_images, val_labels, model\n",
    "gc.collect()  # Manually trigger garbage collection\n",
    "print(\"Memory freed.\")\n",
    "\n",
    "# Output best model\n",
    "best_model = max(results, key=lambda x: results[x][\"Accuracy\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully from NumPy files.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Models directory set up.\n",
      "Training SVM model...\n",
      "SVM model training completed in 558.00s.\n",
      "SVM: Accuracy=0.3710, Train Time=558.00s, Model Size=305.07 KB\n",
      "Training RandomForest model...\n",
      "RandomForest model training completed in 2.02s.\n",
      "RandomForest: Accuracy=0.5434, Train Time=2.02s, Model Size=29.28 KB\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed in 124.57s.\n",
      "XGBoost: Accuracy=0.8607, Train Time=124.57s, Model Size=337.54 KB\n",
      "Best model: XGBoost with Accuracy: 0.8607\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function (further optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float32') / 255.0  # Normalize\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float32)\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully from NumPy files.\")\n",
    "\n",
    "# Apply optimized preprocessing\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(\"Models directory set up.\")\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", MinMaxScaler()), \n",
    "        (\"svm\", LinearSVC(max_iter=3000, tol=1e-4, dual=False))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=15, max_depth=3, min_samples_split=4, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=60, max_depth=3, learning_rate=0.08, \n",
    "        colsample_bytree=0.75, subsample=0.9, reg_alpha=0.01, verbosity=0, eval_metric=\"logloss\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    print(f\"{name} model training completed in {train_time:.2f}s.\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "\n",
    "    # Save model to \"models\" directory\n",
    "    model_path = os.path.join(models_dir, f\"{name}_opt1.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # Convert to KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    \n",
    "    print(f\"{name}: Accuracy={acc:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")\n",
    "\n",
    "# Output best model\n",
    "best_model = max(results, key=lambda x: results[x][\"Accuracy\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison table with the actual results from the optimizations:\n",
    "\n",
    "| Model         | Run | Accuracy | Train Time (s) | Model Size (KB) |\n",
    "|--------------|-----|----------|---------------|-----------------|\n",
    "| **SGD**      | 1   | 0.4384   | 27.55         | 224.35          |\n",
    "| **SVM**      | 1   | 0.4387   | 1291.84       | 326480.53       |\n",
    "| **RandomForest** | 1   | 0.5397   | 3.99          | 87.19           |\n",
    "| **XGBoost**  | 1   | 0.7173   | 35.34         | 115.28          |\n",
    "| **SVM**      | 2   | 0.3812   | 421.04        | 341956.81       |\n",
    "| **RandomForest** | 2   | 0.5397   | 0.28          | 20.41           |\n",
    "| **XGBoost**  | 2   | 0.8665   | 72.21         | 370.22          |\n",
    "| **SVM**      | 3   | 0.3812   | 381.53        | 341956.81       |\n",
    "| **RandomForest** | 3   | 0.5440   | 0.21          | 20.00           |\n",
    "| **XGBoost**  | 3   | 0.8940   | 52.03         | 740.09          |\n",
    "| **SVM**           | 1st Opt. | 0.3667   | 229.88         | 256036.50       |\n",
    "| **RandomForest**  | 1st Opt. | 0.4946   | 0.21           | 10.44           |\n",
    "| **XGBoost**       | 1st Opt. | 0.8360   | 15.84          | 282.03          |\n",
    "| **SVM**           | 2nd Opt. | 0.3670   | 239.53         | 241.32          |\n",
    "| **RandomForest**  | 2nd Opt. | 0.5440   | 0.26           | 28.47           |\n",
    "| **XGBoost**       | 2nd Opt. | 0.8471   | 21.57          | 337.08          |\n",
    "\n",
    "### Best Model\n",
    "\n",
    "The best model to use is **XGBoost**.\n",
    "\n",
    "### Reasons:\n",
    "1. **Highest Accuracy**: XGBoost consistently achieved the highest accuracy across all runs, with the highest accuracy being 0.8940.\n",
    "2. **Reasonable Training Time**: XGBoost has a reasonable training time compared to SVM, which has significantly longer training times.\n",
    "3. **Model Size**: The model size of XGBoost is moderate and manageable compared to the extremely large size of the SVM model.\n",
    "\n",
    "### Considering Accuracy, Time, and Storage:\n",
    "- **Accuracy**: XGBoost has the highest accuracy (0.8940).\n",
    "- **Training Time**: XGBoost has a reasonable training time (15.84s to 72.21s).\n",
    "- **Model Size**: XGBoost has a moderate model size (115.95 KB to 740.09 KB).\n",
    "\n",
    "Overall, XGBoost provides the best balance of accuracy, training time, and model size, making it the most suitable choice for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Tuning to achieve an accurate model to minimize resource use in CubeSat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Training SGD model...\n",
      "SGD: Accuracy=0.3979, F1-Score=0.3867, Train Time=23.39s, Model Size=254.85 KB\n",
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Accuracy=0.3546, F1-Score=0.3542, Train Time=160.34s, Model Size=236544.81 KB\n",
      "Training RandomForest model...\n",
      "RandomForest: Accuracy=0.6867, F1-Score=0.6677, Train Time=2.78s, Model Size=62.55 KB\n",
      "Training XGBoost model...\n",
      "XGBoost: Accuracy=0.8474, F1-Score=0.8311, Train Time=76.86s, Model Size=282.83 KB\n",
      "Best model: XGBoost with Accuracy: 0.8474 and F1-Score: 0.8311\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function (optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = X.astype('float16') / 255.0  # Normalize and reduce memory\n",
    "    if X_pre.ndim == 4 and X_pre.shape[-1] == 3:\n",
    "        X_pre = np.array([rgb2gray(image) for image in X_pre], dtype=np.float16)  # Convert to grayscale\n",
    "    X_pre = np.array([resize(image, size, anti_aliasing=False) for image in X_pre], dtype=np.float16)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1)  # Flatten\n",
    "\n",
    "# Load dataset efficiently\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully.\")\n",
    "\n",
    "# Apply preprocessing\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", MinMaxScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0005, max_iter=500, tol=1e-4, class_weight=\"balanced\", n_jobs=-1))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50, max_iter=1000, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=20, max_depth=4, min_samples_split=5, class_weight=\"balanced\", n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=50, max_depth=3, learning_rate=0.07, colsample_bytree=0.85, subsample=0.95, verbosity=0, use_label_encoder=False\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "    f1 = f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(models_dir, f\"{name}4.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"F1-Score\": f1, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, F1-Score={f1:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")\n",
    "\n",
    "    # Free memory after each model\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Select best model based on F1-Score\n",
    "best_model = max(results, key=lambda x: results[x][\"F1-Score\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f} and F1-Score: {results[best_model]['F1-Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Data loaded successfully.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "Training SGD model...\n",
      "SGD: Accuracy=0.3923, F1-Score=0.3839, Train Time=24.24s, Model Size=204.21 KB\n",
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Accuracy=0.3355, F1-Score=0.3353, Train Time=172.18s, Model Size=235408.95 KB\n",
      "Training RandomForest model...\n",
      "RandomForest: Accuracy=0.6793, F1-Score=0.6623, Train Time=2.66s, Model Size=62.75 KB\n",
      "Training XGBoost model...\n",
      "XGBoost: Accuracy=0.8480, F1-Score=0.8316, Train Time=102.11s, Model Size=282.96 KB\n",
      "Best model: XGBoost with Accuracy: 0.8480 and F1-Score: 0.8316\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function (optimized for minimal memory usage)\n",
    "def preprocessing_fn_ML(X, size=(72, 72)):\n",
    "    print(\"Starting preprocessing...\")\n",
    "    X_pre = np.array([resize(rgb2gray(image), size, anti_aliasing=False) for image in X], dtype=np.float32)\n",
    "    print(\"Preprocessing completed.\")\n",
    "    return X_pre.reshape(len(X_pre), -1) / 255.0  # Flatten and normalize\n",
    "\n",
    "# Load dataset efficiently\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(\"Training Data loaded successfully.\")\n",
    "\n",
    "# Apply preprocessing\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models folder\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", MinMaxScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0005, max_iter=500, tol=1e-4, class_weight=\"balanced\", n_jobs=-1))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=50, max_iter=1000, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=20, max_depth=4, min_samples_split=5, class_weight=\"balanced\", n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=50, max_depth=3, learning_rate=0.07, colsample_bytree=0.85, subsample=0.95, verbosity=0, use_label_encoder=False\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "    f1 = f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(models_dir, f\"{name}5.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"F1-Score\": f1, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, F1-Score={f1:.4f}, Train Time={train_time:.2f}s, Model Size={model_size:.2f} KB\")\n",
    "\n",
    "    # Free memory after each model\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Select best model based on F1-Score\n",
    "best_model = max(results, key=lambda x: results[x][\"F1-Score\"])\n",
    "print(f\"Best model: {best_model} with Accuracy: {results[best_model]['Accuracy']:.4f} and F1-Score: {results[best_model]['F1-Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: Acc=0.3855, F1=0.3719, TrainTime=230.49s, ModelSize=325.60KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=800).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Acc=0.3556, F1=0.3539, TrainTime=148.70s, ModelSize=211718.17KB\n",
      "RandomForest: Acc=0.7278, F1=0.6996, TrainTime=5.36s, ModelSize=135.95KB\n",
      "XGBoost: Acc=0.8418, F1=0.8247, TrainTime=78.32s, ModelSize=227.06KB\n",
      "Best model: XGBoost -> Acc=0.8418, F1=0.8247\n"
     ]
    }
   ],
   "source": [
    "# Optimized preprocessing function with parallelism\n",
    "def process_image(image, size=(72, 72)):\n",
    "    return resize(rgb2gray(image), size, anti_aliasing=False).flatten()\n",
    "\n",
    "def preprocessing_fn_ML(X):\n",
    "    return np.array(Parallel(n_jobs=-1)(delayed(process_image)(img) for img in X), dtype=np.float16)\n",
    "\n",
    "# Load dataset efficiently using mmap_mode\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "\n",
    "# Apply preprocessing\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Define optimized models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0003, max_iter=400, tol=1e-4, class_weight=\"balanced\", n_jobs=-1))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"svm\", SVC(kernel=\"linear\", probability=False, cache_size=100, max_iter=800, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=30, max_depth=5, min_samples_split=5, class_weight=\"balanced\", n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=40, max_depth=3, learning_rate=0.08, colsample_bytree=0.9, subsample=0.9, verbosity=0, use_label_encoder=False, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc = accuracy_score(val_labels, pred_labels)\n",
    "    f1 = f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"models/{name}6.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(model_path) / 1024  # in KB\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"F1-Score\": f1, \"Train Time\": train_time, \"Model Size (KB)\": model_size}\n",
    "    print(f\"{name}: Acc={acc:.4f}, F1={f1:.4f}, TrainTime={train_time:.2f}s, ModelSize={model_size:.2f}KB\")\n",
    "\n",
    "    # Free memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Select best model based on F1-Score\n",
    "best_model = max(results, key=lambda x: results[x][\"F1-Score\"])\n",
    "print(f\"Best model: {best_model} -> Acc={results[best_model]['Accuracy']:.4f}, F1={results[best_model]['F1-Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 9711 training images, 3237 validation images.\n",
      "Preprocessing training images...\n",
      "Processing 9711 images...\n",
      "Preprocessing validation images...\n",
      "Processing 3237 images...\n",
      "\n",
      "Training SGD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training completed in 21.60s.\n",
      "Evaluating SGD model...\n",
      "SGD -> Accuracy: 0.8255, F1-Score: 0.8302, Training Time: 21.60s, Model Size: 47.17KB\n",
      "Memory cleaned up for SGD model.\n",
      "\n",
      "Training RandomForest model...\n",
      "RandomForest training completed in 2.11s.\n",
      "Evaluating RandomForest model...\n",
      "RandomForest -> Accuracy: 0.7164, F1-Score: 0.6852, Training Time: 2.11s, Model Size: 202.89KB\n",
      "Memory cleaned up for RandomForest model.\n",
      "\n",
      "Training XGBoost model...\n",
      "XGBoost training completed in 5.46s.\n",
      "Evaluating XGBoost model...\n",
      "XGBoost -> Accuracy: 0.7461, F1-Score: 0.7103, Training Time: 5.46s, Model Size: 174.91KB\n",
      "Memory cleaned up for XGBoost model.\n",
      "\n",
      "🏆 Best Model: SGD -> Accuracy: 0.8255, F1-Score: 0.8302\n"
     ]
    }
   ],
   "source": [
    "# Optimized image preprocessing with HOG for better feature extraction\n",
    "def process_image(image, size=(72, 72)):\n",
    "    img = resize(rgb2gray(image), size, anti_aliasing=False)\n",
    "    return hog(img, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "\n",
    "def preprocessing_fn_ML(X):\n",
    "    print(f\"Processing {len(X)} images...\")  # Debugging print\n",
    "    return np.array(Parallel(n_jobs=2)(delayed(process_image)(img) for img in X), dtype=np.float16)\n",
    "\n",
    "# Load dataset using mmap_mode to save RAM\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "print(f\"Dataset loaded: {len(train_images)} training images, {len(val_images)} validation images.\")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training images...\")\n",
    "train_images = preprocessing_fn_ML(train_images)\n",
    "print(\"Preprocessing validation images...\")\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Ensure models folder exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Define optimized models with lower CPU usage and improved accuracy\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0002, max_iter=300, tol=1e-4, class_weight=\"balanced\", n_jobs=2))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=20, max_depth=6, min_samples_split=4, class_weight=\"balanced\", n_jobs=2\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=30, max_depth=3, learning_rate=0.07, colsample_bytree=0.85, subsample=0.9, verbosity=0, use_label_encoder=False, n_jobs=2\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"{name} training completed in {train_time:.2f}s.\")\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    print(f\"Evaluating {name} model...\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc, f1 = accuracy_score(val_labels, pred_labels), f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model and get size\n",
    "    model_path = f\"models/{name}7.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    model_size = os.path.getsize(model_path) / 1024  # in KB\n",
    "\n",
    "    results[name] = {\"Acc\": acc, \"F1\": f1, \"Train Time\": train_time, \"Size (KB)\": model_size}\n",
    "    print(f\"{name} -> Accuracy: {acc:.4f}, F1-Score: {f1:.4f}, Training Time: {train_time:.2f}s, Model Size: {model_size:.2f}KB\")\n",
    "\n",
    "    # Free memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(f\"Memory cleaned up for {name} model.\")\n",
    "\n",
    "# Get best model\n",
    "best = max(results, key=lambda x: results[x][\"F1\"])\n",
    "print(f\"\\n🏆 Best Model: {best} -> Accuracy: {results[best]['Acc']:.4f}, F1-Score: {results[best]['F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Applying PCA...\n",
      "Processing 9711 images...\n",
      "Fitting PCA...\n",
      "Processing 3237 images...\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest trained in 4.11s.\n",
      "Evaluating RandomForest...\n",
      "RandomForest: Accuracy=0.8838, F1-Score=0.8812, TrainTime=4.11s\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost trained in 2.35s.\n",
      "Evaluating XGBoost...\n",
      "XGBoost: Accuracy=0.8863, F1-Score=0.8833, TrainTime=2.35s\n",
      "\n",
      " Best Model: XGBoost -> Accuracy: 0.8863, F1-Score: 0.8833\n"
     ]
    }
   ],
   "source": [
    "# Image Preprocessing with HOG & PCA\n",
    "def process_image(image, size=(72, 72)):\n",
    "    img = resize(rgb2gray(image), size, anti_aliasing=False)\n",
    "    return hog(img, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "\n",
    "def preprocessing_fn_ML(X, fit_pca=False):\n",
    "    print(f\"Processing {len(X)} images...\")\n",
    "    features = np.array(Parallel(n_jobs=-1)(delayed(process_image)(img) for img in X), dtype=np.float16)\n",
    "    \n",
    "    if fit_pca:\n",
    "        global pca\n",
    "        print(\"Fitting PCA...\")\n",
    "        pca = PCA(n_components=100)\n",
    "        pca.fit(features)\n",
    "    \n",
    "    return pca.transform(features)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "\n",
    "# Apply PCA on Training Data, then Transform Validation Data\n",
    "print(\"Applying PCA...\")\n",
    "train_images = preprocessing_fn_ML(train_images, fit_pca=True)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Ensure models folder exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Optimized Models\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=4, class_weight=\"balanced\", n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.05, colsample_bytree=0.9, subsample=0.9, verbosity=0, use_label_encoder=False, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Training & Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"{name} trained in {train_time:.2f}s.\")\n",
    "\n",
    "    # Predictions\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc, f1 = accuracy_score(val_labels, pred_labels), f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, f\"models/{name}8.pkl\")\n",
    "\n",
    "    results[name] = {\"Acc\": acc, \"F1\": f1, \"Train Time\": train_time}\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, F1-Score={f1:.4f}, TrainTime={train_time:.2f}s\")\n",
    "\n",
    "    # Free Memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Best Model\n",
    "best = max(results, key=lambda x: results[x][\"F1\"])\n",
    "print(f\"\\n Best Model: {best} -> Accuracy: {results[best]['Acc']:.4f}, F1-Score: {results[best]['F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Applying PCA...\n",
      "Processing 9711 images...\n",
      "Fitting PCA...\n",
      "Processing 3237 images...\n",
      "\n",
      "Training SGD...\n",
      "SGD trained in 1.80s.\n",
      "Evaluating SGD...\n",
      "SGD: Accuracy=0.8221, F1-Score=0.8167, TrainTime=1.80s\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest trained in 4.10s.\n",
      "Evaluating RandomForest...\n",
      "RandomForest: Accuracy=0.8817, F1-Score=0.8791, TrainTime=4.10s\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost trained in 2.33s.\n",
      "Evaluating XGBoost...\n",
      "XGBoost: Accuracy=0.8863, F1-Score=0.8833, TrainTime=2.33s\n",
      "\n",
      " Best Model: XGBoost -> Accuracy: 0.8863, F1-Score: 0.8833\n"
     ]
    }
   ],
   "source": [
    "# Image Preprocessing (HOG + PCA)\n",
    "def process_image(image, size=(72, 72)):\n",
    "    img = resize(rgb2gray(image), size, anti_aliasing=False)\n",
    "    return hog(img, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "\n",
    "def preprocessing_fn_ML(X, fit_pca=False):\n",
    "    print(f\"Processing {len(X)} images...\")\n",
    "    features = np.array(Parallel(n_jobs=-1)(delayed(process_image)(img) for img in X), dtype=np.float16)\n",
    "    \n",
    "    if fit_pca:\n",
    "        global pca\n",
    "        print(\"Fitting PCA...\")\n",
    "        pca = PCA(n_components=100)\n",
    "        pca.fit(features)\n",
    "    \n",
    "    return pca.transform(features)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "\n",
    "# Apply PCA on Training Data, then Transform Validation Data\n",
    "print(\"Applying PCA...\")\n",
    "train_images = preprocessing_fn_ML(train_images, fit_pca=True)\n",
    "val_images = preprocessing_fn_ML(val_images)\n",
    "\n",
    "# Ensure models folder exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Optimized Models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.0001, max_iter=300, tol=1e-4, class_weight=\"balanced\", n_jobs=-1))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=4, class_weight=\"balanced\", n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.05, colsample_bytree=0.9, subsample=0.9, verbosity=0, use_label_encoder=False, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Training & Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"{name} trained in {train_time:.2f}s.\")\n",
    "\n",
    "    # Predictions\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc, f1 = accuracy_score(val_labels, pred_labels), f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, f\"models/{name}9.pkl\")\n",
    "\n",
    "    results[name] = {\"Acc\": acc, \"F1\": f1, \"Train Time\": train_time}\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, F1-Score={f1:.4f}, TrainTime={train_time:.2f}s\")\n",
    "\n",
    "    # Free Memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Best Model\n",
    "best = max(results, key=lambda x: results[x][\"F1\"])\n",
    "print(f\"\\n Best Model: {best} -> Accuracy: {results[best]['Acc']:.4f}, F1-Score: {results[best]['F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Applying PCA...\n",
      "Processing 9711 images...\n",
      "Fitting PCA...\n",
      "Processing 3237 images...\n",
      "\n",
      "Training SGD...\n",
      "SGD trained in 5.54s.\n",
      "Evaluating SGD...\n",
      "SGD: Accuracy=0.8282, F1-Score=0.8254, TrainTime=5.54s\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting trained in 197.06s.\n",
      "Evaluating GradientBoosting...\n",
      "GradientBoosting: Accuracy=0.8984, F1-Score=0.8962, TrainTime=197.06s\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost trained in 3.93s.\n",
      "Evaluating XGBoost...\n",
      "XGBoost: Accuracy=0.8845, F1-Score=0.8812, TrainTime=3.93s\n",
      "\n",
      " Best Model: GradientBoosting -> Accuracy: 0.8984, F1-Score: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# Optimized Image Preprocessing\n",
    "def process_images(X, fit_pca=False):\n",
    "    print(f\"Processing {len(X)} images...\")\n",
    "    features = np.array([hog(resize(rgb2gray(img), (72, 72), anti_aliasing=False),\n",
    "                             pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "                         for img in X], dtype=np.float16)\n",
    "    \n",
    "    global pca\n",
    "    if fit_pca:\n",
    "        print(\"Fitting PCA...\")\n",
    "        pca = PCA(n_components=150)\n",
    "        features = pca.fit_transform(features)\n",
    "    else:\n",
    "        features = pca.transform(features)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Load Dataset\n",
    "print(\"Loading dataset...\")\n",
    "train_images = np.load('data/train_images.npy', mmap_mode='r')\n",
    "train_labels = np.load('data/train_labels.npy', mmap_mode='r')\n",
    "val_images = np.load('data/val_images.npy', mmap_mode='r')\n",
    "val_labels = np.load('data/val_labels.npy', mmap_mode='r')\n",
    "\n",
    "# Apply Preprocessing & PCA\n",
    "print(\"Applying PCA...\")\n",
    "train_images = process_images(train_images, fit_pca=True)\n",
    "val_images = process_images(val_images)\n",
    "\n",
    "# Ensure models folder exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Optimized Models\n",
    "models = {\n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"sgd\", SGDClassifier(loss=\"log_loss\", alpha=0.00005, max_iter=500, tol=1e-5, class_weight=\"balanced\", n_jobs=1))\n",
    "    ]),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=50, learning_rate=0.05, max_depth=4),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.04, colsample_bytree=0.95, subsample=0.95, verbosity=0, use_label_encoder=False, n_jobs=1)\n",
    "}\n",
    "\n",
    "# Training & Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"{name} trained in {train_time:.2f}s.\")\n",
    "\n",
    "    # Predictions\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pred_labels = model.predict(val_images)\n",
    "    acc, f1 = accuracy_score(val_labels, pred_labels), f1_score(val_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model, f\"models/{name}10.pkl\")\n",
    "\n",
    "    results[name] = {\"Acc\": acc, \"F1\": f1, \"Train Time\": train_time}\n",
    "    print(f\"{name}: Accuracy={acc:.4f}, F1-Score={f1:.4f}, TrainTime={train_time:.2f}s\")\n",
    "\n",
    "    # Free Memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Best Model\n",
    "best = max(results, key=lambda x: results[x][\"F1\"])\n",
    "print(f\"\\n Best Model: {best} -> Accuracy: {results[best]['Acc']:.4f}, F1-Score: {results[best]['F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
